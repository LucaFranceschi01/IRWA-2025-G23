{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fe4d8d",
   "metadata": {},
   "source": [
    "# Part 1: Text Processing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b198f",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb905e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a419a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path =  os.path.join(os.getcwd(), '../../data/')\n",
    "doc_path = os.path.join(data_path, 'fashion_products_dataset.json')\n",
    "\n",
    "data = pd.read_json(doc_path)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf2534",
   "metadata": {},
   "source": [
    "1. As a first step, you must pre-process the documents. In particular, for the text fields (title,\n",
    "description) you should:\n",
    "\n",
    "- Removing stop words\n",
    "- Tokenization\n",
    "- Removing punctuation marks\n",
    "- Stemming\n",
    "- and... anything else you think it's needed (bonus point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9628e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Lowercase\n",
    "    text = text.translate(translator) # Remove punctuation\n",
    "    text = unidecode(text) # normalize\n",
    "    tokens = word_tokenize(text) # Tokenization\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words] # Remove stopwords and non-alphabetic tokens\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens] # Stemming \n",
    "    stemmed_tokens = [word for word in stemmed_tokens if len(word) > 2] # Remove short tokens\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "data['title_clean'] = data['title'].apply(preprocess_text)\n",
    "data['description_clean'] = data['description'].apply(preprocess_text)\n",
    "\n",
    "data[['pid', 'title', 'title_clean', 'description_clean']].head(5)\n",
    "\n",
    "# Replace original columns with cleaned versions\n",
    "data['title'] = data['title_clean']\n",
    "data['description'] = data['description_clean']\n",
    "data.drop(columns=['title_clean', 'description_clean'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2e779",
   "metadata": {},
   "source": [
    "2. Take into account that for future queries, the final output must return (when present) the following information for each of the  elected documents: pid, title, description, brand, category, sub_category, product_details, seller, out_of_stock, selling_price, discount, actual_price, average_rating, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\n",
    "    'pid', 'title', 'description', 'brand', \n",
    "    'category', 'sub_category', 'product_details', \n",
    "    'seller', 'out_of_stock', 'selling_price', \n",
    "    'discount', 'actual_price', 'average_rating', 'url'\n",
    "]\n",
    "\n",
    "available_fields = [f for f in relevant_columns if f in data.columns]\n",
    "clean_data = data[available_fields].copy()\n",
    "clean_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecee2ad",
   "metadata": {},
   "source": [
    "3. Decide how to handle the fields category, sub_category, brand, product_details, and seller during pre-processing. Should they be merged into a single text field, indexed as separate fields in the inverted index or any other alternative? Justify your choice, considering how their distinctiveness may affect retrieval effectiveness. What are pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f819bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metadata_field(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return text.strip()\n",
    "\n",
    "metadata_fields = ['category', 'sub_category', 'brand', 'product_details', 'seller']\n",
    "for field in metadata_fields:\n",
    "    clean_data[f'{field}_clean'] = clean_data[field].apply(clean_metadata_field)\n",
    "\n",
    "display(clean_data.head(5))\n",
    "\n",
    "# Replace original metadata columns with cleaned versions\n",
    "for field in metadata_fields:\n",
    "    clean_data[field] = clean_data[f'{field}_clean']\n",
    "    clean_data.drop(columns=[f'{field}_clean'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd9f47",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5c438",
   "metadata": {},
   "source": [
    "4. Consider the fields out_of_stock, selling_price, discount, actual_price, and average_rating. Decide how these should be handled during pre-processing to use in further search. Should they be indexed as textual terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b065bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields = ['selling_price', 'actual_price', 'average_rating']\n",
    "\n",
    "# Treat discount separately to handle percentage signs\n",
    "data['discount'] = data['discount'].str.replace('% off', '', regex=False)  # Remove literal string\n",
    "data['discount'] = pd.to_numeric(data['discount'], errors='coerce')\n",
    "\n",
    "# Convert to binary (0/1) for out_of_stock\n",
    "data['out_of_stock'] = data['out_of_stock'].map({True: 1, False: 0})\n",
    "\n",
    "# Convert numerical fields to appropriate types\n",
    "for field in numerical_fields:\n",
    "    data[field] = pd.to_numeric(data[field], errors='coerce')\n",
    "\n",
    "numerical_fields = ['out_of_stock', 'discount', 'selling_price', 'actual_price', 'average_rating']\n",
    "\n",
    "display(data[numerical_fields].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642171d5",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1924d",
   "metadata": {},
   "source": [
    "When working with data, it is important to have a better understanding of the content and some statistics. Provide an exploratory data analysis to describe the dataset you are working on in this project and explain the decisions made for the analysis. For example, word counting distribution, average sentence length, vocabulary size, ranking of products based on rating, price, discount, top sellers and brands, out_of_stock distribution, word clouds for the most frequent words, and entity recognition. Feel free to do the exploratory analysis and report your findings in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e5a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
