{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d87ec56",
   "metadata": {},
   "source": [
    "# Part 2: Indexing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11f0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c526c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "data = pd.read_csv('../part_1/fashion_products_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc861bf",
   "metadata": {},
   "source": [
    "## 1. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88c140",
   "metadata": {},
   "source": [
    "### 1.1 Build inverted index\n",
    "\n",
    "After having pre-processed the data, you can then create the inverted index.\n",
    "\n",
    "HINT - you may use the vocabulary data structure, like the one seen during the\n",
    "Practical Labs:\n",
    "\n",
    "{\n",
    "    Term_id_1: [document_1, document_2, document_4],\n",
    "    Term_id_2: [document_1, document_3, document_5, document_6],\n",
    "    etcâ€¦\n",
    "}\n",
    "\n",
    "Important: For this assignment, we will be using conjunctive queries (AND). This means that every returned document must contain all the words from the query in order to be considered a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfeaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23de755",
   "metadata": {},
   "source": [
    "### 1.2 Propose test queries\n",
    "\n",
    "Define five queries that will be used to evaluate your search engine. (Be creative ðŸ˜‰)\n",
    "\n",
    "HINT: How to choose the queries? The selection of the queries is up to you, but itâ€™s suggested to select terms based on the popularity (keywords ranked by term frequencies or by TF-IDF, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5703ea",
   "metadata": {},
   "source": [
    "### 1.3 Rank your results\n",
    "\n",
    "Implement the TF-IDF algorithm and provide ranking-based results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4ac44",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e85cfc6",
   "metadata": {},
   "source": [
    "### 2.1 Implement the following evaluation metrics to assess the effectiveness of your retrieval solutions. \n",
    "\n",
    "These metrics will help you measure how well your system retrieves relevant documents for each query:\n",
    "\n",
    "i. Precision@K (P@K)\n",
    "\n",
    "ii. Recall@K (R@K)\n",
    "\n",
    "iii. Average Precision@K (P@K)\n",
    "\n",
    "iv. F1-Score@K\n",
    "\n",
    "v. Mean Average Precision (MAP)\n",
    "\n",
    "vi. Mean Reciprocal Rank (MRR)\n",
    "\n",
    "vii. Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2d3af",
   "metadata": {},
   "source": [
    "### 2.2 Apply Evaluation Metrics\n",
    "Apply the evaluation metrics you have implemented to the search results and relevance judgments provided in validation_labels.csv for the predefined queries. When reporting evaluation results, provide only numeric values, rounded to three decimal places. Do not include textual explanations or additional statistics in this section.\n",
    "\n",
    "a. Query 1: women full sleeve sweatshirt cotton\n",
    "\n",
    "b. Query 2: men slim jeans blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289b6fd",
   "metadata": {},
   "source": [
    "### 2.3 Relevance Judgments and Analysis\n",
    "\n",
    "You will act as expert judges by establishing the ground truth for each document and query.\n",
    "\n",
    "a. For the test queries you defined in Part 1, Step 2 during indexing, assign a binary relevance label to each document: 1 if the document is relevant to the query, or 0 if it is not.\n",
    "\n",
    "b. Comment on each of the evaluation metrics, stating how they differ, and which information gives each of them. Analyze your results.\n",
    "\n",
    "c. Analyze the current search system and identify its main problems or limitations. For each issue you find, propose possible ways to resolve it. Consider aspects such as retrieval accuracy, ranking quality, handling of different field types, query formulation, and indexing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
