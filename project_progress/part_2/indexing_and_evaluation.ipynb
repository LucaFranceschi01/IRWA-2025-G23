{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d87ec56",
   "metadata": {},
   "source": [
    "# Part 2: Indexing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, collections, string, re, math\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c526c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "data_path =  os.path.join(os.getcwd(), '../../data/')\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_path, 'fashion_products_cleaned.csv'))\n",
    "# text_columns = ['title', 'description']\n",
    "# text_columns = ['title', 'description', 'brand', 'category', 'sub_category', 'seller']\n",
    "text_columns = ['title']\n",
    "data[text_columns] = data[text_columns].fillna('')\n",
    "\n",
    "data[2640:2660]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc861bf",
   "metadata": {},
   "source": [
    "## 1. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88c140",
   "metadata": {},
   "source": [
    "### 1.1 Build inverted index\n",
    "\n",
    "After having pre-processed the data, you can then create the inverted index.\n",
    "\n",
    "HINT - you may use the vocabulary data structure, like the one seen during the\n",
    "Practical Labs:\n",
    "\n",
    "{\n",
    "    Term_id_1: [document_1, document_2, document_4],\n",
    "    Term_id_2: [document_1, document_3, document_5, document_6],\n",
    "    etcâ€¦\n",
    "}\n",
    "\n",
    "Important: For this assignment, we will be using conjunctive queries (AND). This means that every returned document must contain all the words from the query in order to be considered a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cdb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function used in PART-1\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Lowercase\n",
    "    text = text.translate(translator) # Remove punctuation\n",
    "    text = unidecode(text) # normalize\n",
    "    tokens = word_tokenize(text) # Tokenization\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words] # Remove stopwords and non-alphabetic tokens\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens] # Stemming \n",
    "    stemmed_tokens = [word for word in stemmed_tokens if len(word) > 2] # Remove short tokens\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cedf48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Combining text fields from the dataset to create a searchable document for each product.\n",
    "def create_document_for_each_row(row, text_columns):\n",
    "    ''' \n",
    "    Create a single text document for each row by concatenating specified text fields\n",
    "    Returns: string of concatenated text fields\n",
    "    '''\n",
    "    text_fields = [str(row[col]) for col in text_columns if pd.notnull(row[col])]\n",
    "    document = ' '.join([f for f in text_fields if f != 'nan' and f != '']).lower()\n",
    "    return document\n",
    "\n",
    "# 2. Building an inverted index to map terms to product IDs.\n",
    "def build_inverted_index(data, text_columns):\n",
    "    '''\n",
    "    Build an inverted index from the dataset\n",
    "    Returns: dict mapping terms to list of document IDs\n",
    "    '''\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        doc_id = row['pid']\n",
    "        row_text = create_document_for_each_row(row, text_columns) \n",
    "\n",
    "        ####### SI?\n",
    "        row_text = ' '.join(preprocess_text(row_text))\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = re.findall(r'\\b\\w+\\b', row_text)\n",
    "\n",
    "        # Add to inverted index (avoindin duplicates)\n",
    "        already_seen_terms = set()\n",
    "        for term in tokens:\n",
    "            if term not in already_seen_terms:\n",
    "                inverted_index[term].append(doc_id)\n",
    "                already_seen_terms.add(term)\n",
    "\n",
    "    return dict(inverted_index)\n",
    "\n",
    "# 3. Implementing a simple search function to retrieve products based on keyword queries. (conjunctive queries)\n",
    "def conjunctive_search(query, inverted_index):\n",
    "    '''\n",
    "    Perform AND query: return documents containing ALL QUERY TERMS\n",
    "    Returns: list of document IDs\n",
    "    '''\n",
    "    query_terms = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    if not query_terms:\n",
    "        return []\n",
    "\n",
    "    # Get documents for the first term\n",
    "    if query_terms[0] in inverted_index:\n",
    "        result_docs = set(inverted_index[query_terms[0]])\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    # Intersect with documents for the remaining terms\n",
    "    for term in query_terms[1:]:\n",
    "        if term in inverted_index:\n",
    "            result_docs = result_docs.intersection(set(inverted_index[term]))\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    return list(result_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94edaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: detail_columns = ['detail_fabric', 'detail_color', 'detail_pattern', ...]\n",
    "# For now we are building it without the details columns\n",
    "\n",
    "# --- Our inverted index ---\n",
    "inverted_index = build_inverted_index(data, text_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23de755",
   "metadata": {},
   "source": [
    "### 1.2 Propose test queries\n",
    "\n",
    "Define five queries that will be used to evaluate your search engine. (Be creative ðŸ˜‰)\n",
    "\n",
    "HINT: How to choose the queries? The selection of the queries is up to you, but itâ€™s suggested to select terms based on the popularity (keywords ranked by term frequencies or by TF-IDF, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {\n",
    "    1: 'men cotton shirt',\n",
    "    2: 'women casual polo neck',\n",
    "    3: 'men regular fit tshirt',\n",
    "    4: 'zipper sweater',\n",
    "    5: 'solid round neck cotton'\n",
    "}\n",
    "\n",
    "for _, query_text in test_queries.items():\n",
    "    results = conjunctive_search(query_text, inverted_index)\n",
    "    print(f'\\nTest query: \"{query_text}\"')\n",
    "    print(f'Found {len(results)} matching documents')\n",
    "    print(f'Sample results: {results[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5703ea",
   "metadata": {},
   "source": [
    "### 1.3 Rank your results\n",
    "\n",
    "Implement the TF-IDF algorithm and provide ranking-based results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(data, columns=['title', 'description', 'category']):\n",
    "    '''\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "\n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    num_documents -- total number of documents\n",
    "\n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    '''\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
    "    idf = defaultdict(float)\n",
    "    N = len(data.index)\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        \n",
    "        page_id = row['pid']\n",
    "        terms = preprocess_text(' '.join(row[columns].values))\n",
    "\n",
    "        ## ===============================================================\n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and its text is\n",
    "        ##'web retrieval information retrieval':\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0,\n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term] = [page_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        # normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document.\n",
    "            # posting ==> [current_doc, [list of positions]]\n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1]) / norm, 4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "    # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "    # Note: It is computed later after we know the df.\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(N / df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97424411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, tf, idf, score_foat_precision=7):\n",
    "    '''\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "\n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    tf -- term frequencies\n",
    "    idf -- inverted document frequencies\n",
    "\n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    '''\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms\n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query.\n",
    "    # Example: collections.Counter(['hello','hello','world']) --> Counter({'hello': 2, 'world': 1})\n",
    "    # HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]= query_terms_count[term] / query_norm * idf[term] #query_vector[0] corresponds to the first term in the query\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term 'term' in the doc 26\n",
    "            if doc in docs: #if the odcument is in the list of documents retrieved (matching the query)\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "\n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(doc_scores) == 0:\n",
    "        print('No results found, try again')\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index, tf, idf)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return doc_scores\n",
    "\n",
    "def search_tf_idf(query, index, tf, idf):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms.\n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query = preprocess_text(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        \n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain 'term'\n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "\n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(set(term_docs))\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, tf, idf)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Our TF-IDF index ---\n",
    "inverted_index, tf_index, df_index, idf_index = create_index_tfidf(data, text_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints done with copilot\n",
    "for qid, query_text in test_queries.items():\n",
    "    ranked_results = search_tf_idf(query_text, inverted_index, tf_index, idf_index)\n",
    "    \n",
    "    print(f'\\nTest query: \"{query_text}\"')\n",
    "    print(f'Found {len(ranked_results)} ranked documents')\n",
    "    \n",
    "    if ranked_results:\n",
    "        print(f'Top 5 results:')\n",
    "        for rank, (score, doc_id) in enumerate(ranked_results[:5], 1):\n",
    "            # Retrieve product information for display\n",
    "            product = data[data['pid'] == doc_id].iloc[0]\n",
    "            title = product['title']\n",
    "            if len(str(title)) > 50:\n",
    "                title = str(title)[:50] + '...'\n",
    "            \n",
    "            print(f'  {rank}. PID: {doc_id} | Score: {score}')\n",
    "            print(f'     Title: {title}')\n",
    "    else:\n",
    "        print('  No results found')\n",
    "    print('-' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4ac44",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e85cfc6",
   "metadata": {},
   "source": [
    "### 2.1 Implement the following evaluation metrics to assess the effectiveness of your retrieval solutions. \n",
    "\n",
    "These metrics will help you measure how well your system retrieves relevant documents for each query:\n",
    "\n",
    "i. Precision@K (P@K)\n",
    "\n",
    "ii. Recall@K (R@K)\n",
    "\n",
    "iii. Average Precision@K (P@K)\n",
    "\n",
    "iv. F1-Score@K\n",
    "\n",
    "v. Mean Average Precision (MAP)\n",
    "\n",
    "vi. Mean Reciprocal Rank (MRR)\n",
    "\n",
    "vii. Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define functions for each evaluation metric.\n",
    "\n",
    "# Precision is the share of retrieved documents that are relevant.\n",
    "def precision_at_k(ranked_docs: pd.Series, val_data: pd.Series, k):\n",
    "    retrieved = ranked_docs[ranked_docs['rank'] <= k]['pid'].values\n",
    "    relevant = val_data[val_data['labels'] == 1]['pid'].values\n",
    "    not_relevant = val_data[val_data['labels'] == 0]['pid'].values\n",
    "\n",
    "    tp = len(set(retrieved) & set(relevant))\n",
    "    fp = len(set(retrieved) & set(not_relevant))\n",
    "\n",
    "    if tp == 0 and fp == 0:\n",
    "        return 0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "# Recall is the share of relevant documents that are retrieved.\n",
    "def recall_at_k(ranked_docs: pd.Series, val_data: pd.Series, k):\n",
    "    retrieved = ranked_docs[ranked_docs['rank'] <= k]['pid'].values\n",
    "    not_retrieved = ranked_docs[ranked_docs['rank'] > k]['pid'].values\n",
    "    relevant = val_data[val_data['labels'] == 1]['pid'].values\n",
    "\n",
    "    tp = len(set(retrieved) & set(relevant))\n",
    "    fn = len(set(not_retrieved) & set(relevant))\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "# Average Precision is the average of precision scores at each rank position where a relevant document is found\n",
    "def average_precision_at_k(ranked_docs: pd.Series, val_data: pd.Series, n):\n",
    "    # slightly different from the one seen in class but it works for our data\n",
    "    gtp = 0\n",
    "    prec_at_i_list = []\n",
    "    prec_at_i_list_debug = []\n",
    "    prev_precision = -1\n",
    "    for k in range(1, n+1):\n",
    "        retrieved = ranked_docs[ranked_docs['rank'] <= k]['pid'].values\n",
    "        relevant = val_data[val_data['labels'] == 1]['pid'].values\n",
    "\n",
    "        # always increases, last value is saved\n",
    "        gtp = len(set(retrieved) & set(relevant))\n",
    "\n",
    "        current_precision = precision_at_k(ranked_docs, val_data, k)\n",
    "        # this if statement does the function of rel@K in the evaluation lab (essentially the same)\n",
    "        prec_at_i_list_debug.append(current_precision)\n",
    "        if current_precision < prev_precision:\n",
    "            prec_at_i_list.append(0)\n",
    "            prev_precision = 0\n",
    "        elif current_precision == prev_precision: # weird case when precision is always 1 bc too few results returned, shall not update otherwise enters a loop of counting precision=1 once every two iterations\n",
    "            prec_at_i_list.append(0)\n",
    "        else:\n",
    "            prec_at_i_list.append(current_precision)\n",
    "            prev_precision = current_precision\n",
    "            \n",
    "    return float((1 / gtp) * np.sum(prec_at_i_list))\n",
    "\n",
    "# F1-score is the harmonic mean of precision and recall.\n",
    "def f1_score_at_k(ranked_docs: pd.Series, val_data: pd.Series, k):\n",
    "    prec = precision_at_k(ranked_docs, val_data, k)\n",
    "    rec = recall_at_k(ranked_docs, val_data, k)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "# Normalized Discounted Cumulative Gain (NDCG) measures the graded relevance of the retrieved documents.\n",
    "def ndcg_at_k(ranked_docs: pd.Series, val_data: pd.Series, k):\n",
    "    retrieved = ranked_docs[ranked_docs['rank'] <= k]['pid'].values\n",
    "    relevant = val_data[val_data['labels'] == 1]['pid'].values\n",
    "\n",
    "    relevance_scores = [1 if doc in relevant else 0 for doc in retrieved] # binary relevance even if we could do more levels of relevance\n",
    "    ideal_list = sorted(relevance_scores, reverse=True)\n",
    "    dcg = sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevance_scores))\n",
    "    ideal_dcg = sum(rel / np.log2(idx + 2) for idx, rel in enumerate(ideal_list))\n",
    "    return float(dcg / ideal_dcg) if ideal_dcg > 0 else 0.0\n",
    "\n",
    "def rr_at_k(ranked_docs: pd.Series, val_data: pd.Series, k):\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for current query\n",
    "    '''\n",
    "\n",
    "    sCorrect_Ri = val_data.merge(ranked_docs, on='pid', how='left')['rank'].min()\n",
    "    \n",
    "    if sCorrect_Ri > k:\n",
    "        return 0\n",
    "    \n",
    "    return 1 / float(sCorrect_Ri)\n",
    "\n",
    "# Mean Average Precision (MAP) is the mean of average precision scores across multiple queries.\n",
    "def mean_average_precision(results: dict):\n",
    "    ap_scores = []\n",
    "    for _, query_result in results.items():\n",
    "        ap_scores.append(query_result['AveragePrecision@K'])\n",
    "\n",
    "    return np.mean(ap_scores)\n",
    "\n",
    "# Mean Reciprocal Rank (MRR) is the average of the reciprocal ranks of the first relevant document across multiple queries.\n",
    "def mean_reciprocal_rank(results: dict):\n",
    "    rr_scores = []\n",
    "    for _, query_result in results.items():\n",
    "        rr_scores.append(query_result['RR@K'])\n",
    "\n",
    "    return np.mean(rr_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2d3af",
   "metadata": {},
   "source": [
    "### 2.2 Apply Evaluation Metrics\n",
    "Apply the evaluation metrics you have implemented to the search results and relevance judgments provided in validation_labels.csv for the predefined queries. When reporting evaluation results, provide only numeric values, rounded to three decimal places. Do not include textual explanations or additional statistics in this section.\n",
    "\n",
    "a. Query 1: women full sleeve sweatshirt cotton\n",
    "\n",
    "b. Query 2: men slim jeans blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(os.path.join(data_path, 'validation_labels.csv'))\n",
    "display(val_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adad177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess the query the same way we did the for full data\n",
    "queries = {\n",
    "    1: 'women full sleeve sweatshirt cotton',\n",
    "    2: 'men slim jeans blue'\n",
    "}\n",
    "\n",
    "k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d83250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of relevant product IDs\n",
    "queries_relevant = val_data[val_data['labels'] == 1]\n",
    "\n",
    "print('Relevant queries: ')\n",
    "display(queries_relevant)\n",
    "\n",
    "# Retrieve and rank documents for each query\n",
    "queries_retrieved = pd.DataFrame(columns=['pid'] + text_columns + ['score', 'query_id'])\n",
    "for qid, query_text in queries.items():\n",
    "    ranked_results = search_tf_idf(query_text, inverted_index, tf_index, idf_index)\n",
    "    ranked_results_df = pd.DataFrame(np.column_stack((ranked_results, np.full(len(ranked_results), qid))), columns=['score', 'pid', 'query_id'])\n",
    "    ranked_results_df = ranked_results_df.merge(data[['pid'] + text_columns], on='pid', how='inner')\n",
    "    queries_retrieved = pd.concat((queries_retrieved, ranked_results_df)).reset_index(drop=True)\n",
    "\n",
    "queries_retrieved['query_id'] = queries_retrieved['query_id'].astype(int)\n",
    "queries_retrieved['score'] = queries_retrieved['score'].astype(float)\n",
    "\n",
    "queries_retrieved.loc[queries_retrieved['query_id'] == 1, 'rank'] = queries_retrieved[queries_retrieved['query_id'] == 1]['score'].rank(method='dense', ascending=False)\n",
    "queries_retrieved.loc[queries_retrieved['query_id'] == 2, 'rank'] = queries_retrieved[queries_retrieved['query_id'] == 2]['score'].rank(method='dense', ascending=False)\n",
    "\n",
    "print('\\nRetrieved queries: ')\n",
    "display(queries_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bfb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.merge(queries_retrieved, on='pid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ccefd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried this probabilistic approach to fix rankings with same score but failed miserably\n",
    "'''\n",
    "best_rand_seeds = {1: [-1, -1], 2: [-1, -1]}\n",
    "for qid in queries:\n",
    "    retrieved = queries_retrieved[queries_retrieved['query_id'] == qid]\n",
    "    relevant = queries_relevant[queries_relevant['query_id'] == qid]\n",
    "\n",
    "    for rand_seed in range(0, 10000):\n",
    "        np.random.seed(rand_seed)\n",
    "        retrieved['score_randomized'] = retrieved['score'] + np.random.normal(0, 0.0000001, retrieved['score'].size)\n",
    "        current_precision = precision_at_k(retrieved, relevant, k)\n",
    "        if best_rand_seeds[qid][1] < current_precision and current_precision > 0.0:\n",
    "            best_rand_seeds[qid][1] = current_precision\n",
    "            best_rand_seeds[qid][0] = rand_seed\n",
    "\n",
    "pprint(best_rand_seeds)\n",
    "'''\n",
    "# will try the ranking approach using pandas.DataFrame.rank\n",
    "# NOTE: this is only because the labels we have are very limited and since there are too many equal-scored documents\n",
    "# the precision is really bad, so we need a way to quantify that the ranking works with the validation data we have available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c792cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for qid in queries:\n",
    "    retrieved = queries_retrieved[queries_retrieved['query_id'] == qid]\n",
    "    val_data_query = val_data[val_data['query_id'] == qid]\n",
    "\n",
    "    results[qid] = {\n",
    "        'Precision@K': precision_at_k(retrieved, val_data_query, k),\n",
    "        'Recall@K': recall_at_k(retrieved, val_data_query, k),\n",
    "        'AveragePrecision@K': average_precision_at_k(retrieved, val_data_query, k),\n",
    "        'F1Score@K': f1_score_at_k(retrieved, val_data_query, k),\n",
    "        'NDCG@K': ndcg_at_k(retrieved, val_data_query, k),\n",
    "        'RR@K': rr_at_k(retrieved, val_data_query, k)\n",
    "    }\n",
    "\n",
    "map_score = mean_average_precision(results)\n",
    "mrr_score = mean_reciprocal_rank(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97642a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid, result in results.items():\n",
    "    print(f'Query {qid} \"{queries[qid]}\":\\n')\n",
    "    for metric, value in result.items():\n",
    "        print(f'\\t\"{metric}\": {value:.4f}')\n",
    "    print()\n",
    "\n",
    "print(f'\\nMAP: {map_score:.4f}')\n",
    "print(f'MRR: {mrr_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289b6fd",
   "metadata": {},
   "source": [
    "### 2.3 Relevance Judgments and Analysis\n",
    "\n",
    "You will act as expert judges by establishing the ground truth for each document and query.\n",
    "\n",
    "a. For the test queries you defined in Part 1, Step 2 during indexing, assign a binary relevance label to each document: 1 if the document is relevant to the query, or 0 if it is not.\n",
    "\n",
    "b. Comment on each of the evaluation metrics, stating how they differ, and which information gives each of them. Analyze your results.\n",
    "\n",
    "c. Analyze the current search system and identify its main problems or limitations. For each issue you find, propose possible ways to resolve it. Consider aspects such as retrieval accuracy, ranking quality, handling of different field types, query formulation, and indexing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49116c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create the general layout for the validation data, edited after.\n",
    "queries_retrieved = pd.DataFrame(columns=['pid', 'title', 'description', 'brand', 'seller'] + ['score', 'query_id'])\n",
    "for qid, query_text in test_queries.items():\n",
    "    ranked_results = search_tf_idf(query_text, inverted_index, tf_index, idf_index)[:20]\n",
    "    ranked_results_df = pd.DataFrame(np.column_stack((ranked_results, np.full(len(ranked_results), qid))), columns=['score', 'pid', 'query_id'])\n",
    "    ranked_results_df = ranked_results_df.merge(data[['pid', 'title', 'description', 'brand']], on='pid', how='inner')\n",
    "    queries_retrieved = pd.concat((queries_retrieved, ranked_results_df)).reset_index(drop=True)\n",
    "\n",
    "# queries_retrieved['labels'] = np.ones(queries_retrieved['pid'].size, dtype=int)\n",
    "# queries_retrieved[['title', 'pid', 'query_id', 'labels']].to_csv(os.path.join(data_path, 'validation_labels2.csv'), index=False)\n",
    "queries_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data2 = pd.read_csv(os.path.join(data_path, 'validation_labels2.csv'))\n",
    "display(val_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ccc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of relevant product IDs\n",
    "queries_relevant = val_data2[val_data2['labels'] == 1]\n",
    "\n",
    "print('Relevant queries: ')\n",
    "display(queries_relevant)\n",
    "\n",
    "# Retrieve and rank documents for each query\n",
    "queries_retrieved = pd.DataFrame(columns=['pid'] + text_columns + ['score', 'query_id'])\n",
    "for qid, query_text in test_queries.items():\n",
    "    ranked_results = search_tf_idf(query_text, inverted_index, tf_index, idf_index)\n",
    "    ranked_results_df = pd.DataFrame(np.column_stack((ranked_results, np.full(len(ranked_results), qid))), columns=['score', 'pid', 'query_id'])\n",
    "    ranked_results_df = ranked_results_df.merge(data[['pid'] + text_columns], on='pid', how='inner')\n",
    "    queries_retrieved = pd.concat((queries_retrieved, ranked_results_df)).reset_index(drop=True)\n",
    "\n",
    "queries_retrieved['query_id'] = queries_retrieved['query_id'].astype(int)\n",
    "queries_retrieved['score'] = queries_retrieved['score'].astype(float)\n",
    "\n",
    "for qid in test_queries:\n",
    "    queries_retrieved.loc[queries_retrieved['query_id'] == qid, 'rank'] = queries_retrieved[queries_retrieved['query_id'] == qid]['score'].rank(method='dense', ascending=False)\n",
    "\n",
    "print('\\nRetrieved queries: ')\n",
    "display(queries_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for qid in test_queries:\n",
    "    retrieved = queries_retrieved[queries_retrieved['query_id'] == qid]\n",
    "    val_data_query = val_data2[val_data2['query_id'] == qid]\n",
    "\n",
    "    results[qid] = {\n",
    "        'Precision@K': precision_at_k(retrieved, val_data_query, k),\n",
    "        'Recall@K': recall_at_k(retrieved, val_data_query, k),\n",
    "        'AveragePrecision@K': average_precision_at_k(retrieved, val_data_query, k),\n",
    "        'F1Score@K': f1_score_at_k(retrieved, val_data_query, k),\n",
    "        'NDCG@K': ndcg_at_k(retrieved, val_data_query, k),\n",
    "        'RR@K': rr_at_k(retrieved, val_data_query, k)\n",
    "    }\n",
    "\n",
    "map_score = mean_average_precision(results)\n",
    "mrr_score = mean_reciprocal_rank(results)\n",
    "\n",
    "for qid, result in results.items():\n",
    "    print(f'Query {qid} \"{test_queries[qid]}\":\\n')\n",
    "    for metric, value in result.items():\n",
    "        print(f'\\t\"{metric}\": {value:.4f}')\n",
    "    print()\n",
    "\n",
    "print(f'\\nMAP: {map_score:.4f}')\n",
    "print(f'MRR: {mrr_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ddb94",
   "metadata": {},
   "source": [
    "We can see that, in general, the precision seems high, while the average precision is kind of low. That is because on how our precision and average precision functions work (the following generally applies to all metrics). Since the documents are quite short in relation to the vocabulary length, when ranking with TF-IDF, there are a ton of repeated scores. That makes it impossible to rank anything fairly. We've decided the following approach: if the score is the same, the rank has to also be the same. Doing that in a practical scenario is probably a bad idea (you cannot show multiple documents in a signle slot in a search engine). However since we are dealing with very limited validation data labels, we need to be able to (at least) get some of the ranked documents \"on top\" without introducing any bias. Using this approach, most of the metrics' logic seems to fall apart, so we had to adapt them. One example is the precision and average precision.\n",
    "\n",
    "Precision takes into account all results that have rank <=20 (which can be tons of results, not only 20, since there are a lot of repeated scores), but apart from that it is quite straight-forward. As for the average precision, we had to adapt the concept of the relevance function since for a single rank we can have multiple documents. To achieve that, we have leveraged the fact that P@k is zero when an error is made. When an error is made, the precision at k will be always lower or equal to the precision at k-1. We can know when that happens by taking into account the current and previous precision.\n",
    "\n",
    "Knowing how average precision works, we can now understand that, even if the precision is high (due to the fact that there are very few correctly ranked results), AP will not take those precisions into account since they are always the same. This concept explains quite well why precision is not a good metric, since you can always have precision=1 if you return only one correct document in the top K results. However, in that same scenario, because of the relevance function, the average precision will be very low (since only a few of returned documents are relevant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c759d2",
   "metadata": {},
   "source": [
    "Apart from that, we can see that recall is always 1 because we have retrieved the validation labels from the results of this ranking and then we've modified its labels (similar thing happens with RR). Also F1 scores are relatively high since both P@K and R@K are high. NDCG values are also high because documents ranked with higher scores are usually tagged as relevant (we used binary relevance since having multiple levels of relevance would complicate this step a ton with how we've defined ranks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
